# deep-learning_from_scratch

0から学ぶディープラーニング
【キーワード】
1章　Python入門

2章　パーセプトロン

3章　ニューラルネットワーク
パラメータへの重み付け　非線形　入力層　中間層　出力層　活性化関数　シグモイド関数　多次元配列の計算　行列の内積計算　信号の伝達　分類問題　回帰問題　ソフトマックス関数　訓練ラベル　テストラベル　推論　バッチ処理

4章　ニューラルネットワークの学習
損失関数　勾配法　データ駆動　特徴量　２乗和誤差　交差エントロピー誤差　ミニバッチ学習　数値微分　偏微分　学習率　ハイパーパラメータ　データの数値変換　

5章　誤差逆伝播法
連鎖律　合成関数　加算　乗算　

6章　学習に関するテクニック
パラメータの最適化　確率的勾配降下法(stochastic gradient descent) AdaGrad Momentum Adam 
他にも過学習などあるがそこまで今回は学べなかった

7章　畳み込みニューラルネットワーク
8章　ディープラーニング

【内容】
1章　Python入門

2章　パーセプトロン

3章　ニューラルネットワーク
パーセプトロンとニューラルネットワーク、この二つは似て非なるもの。線形か非線形か。
この非線型が大事。ニューラルネットワークで、情報を分類したり、未来予測をするにあたり大事。

パラメータ世界に学習に用いるデータをどんどんプロットしていき、新規に入ってきたデータがどこに分類するかを判定して行く。
シグモイド関数やソフトマックス関数など様々な関数が出てきたが、それらの目的は、
x,y軸上にデータをプロットし、決められた範囲の中に世界を映し出し、解を導くディープラーニングを助けてくれるため。

4章　ニューラルネットワークの学習

5章　誤差逆伝播法

6章　学習に関するテクニック

7章　畳み込みニューラルネットワーク

8章　ディープラーニング

【所感】
今回、この本を読んで理解したことは、２つ
⒈ ディープラーニングには学習のフェーズと推論のフェーズがあること
⒉ 学習でも推論でもディープラーニングで行なっていることは決められたグラフの中で非線形の中から答えを導くこと

⒈ 推論について
ディープラーニングの推論では、入力データを関数で伝達して行くことにより、解きたい問題の形に合わせて出力層を人間が設計すれば、機械の方で、解を出してくれることを学んだ。
ディープラーニングが推論で実施していることは、入力データ(音声でも画像でも光でも数値化できるもの全てを実数の範囲内にプロットし、その入力データを重みと行列計算することにより、次の層に伝達。伝達先で先の層にさらに伝達するため、計算できるために活性化関数を用いて変換する、それを繰り返し、答えを返してくれること。

ディープラーニングにできる答えには数種類しかなく、それは、単一分類問題、確率的分類問題、回帰問題である。
単一問題では答えはyes/noの２択で導かれる。その際にはシグモイド関数が計算に用いられる
確率的分類問題では答えは、複数のうちからどの答えが一番確率が高いかで導かれる。その際にはソフトマックス関数が用いられる
回帰問題では、株価のデータなどの未来予測を表示する。その際には計算結果をそのまま表示する恒等関数が用いられる。

2. 学習について
上の推論はこの学習が行われて初めて実現できる。学習とは、訓練データ（教師データ）から最適なパラメータを自動的に獲得することを損失関数を少なくすることを目指して達成しようとすることだと学んだ。


